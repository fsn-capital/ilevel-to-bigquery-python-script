{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fsn-capital/ilevel-to-bigquery-python-script/blob/main/ilevel_to_bigquery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJNTpEa63ILZ"
      },
      "source": [
        "# iLevel to BigQuery"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information\n",
        "This script reads all data from ilevel and pushes it to bigquery.\n",
        "`fetch_method = \"full\"` will refresh all data, and takes about 1h to run (2022 numbers with 2.000.000 records). This is run in Google Colab or similar.\n",
        "`fetch_method = \"append\"` only updates new record since the last run. This is run in GCP Cloud Functions, triggered by a pub/sub topic. As Cloud Functions has a limitation on max 10 minutes per script, the append works fine (less than 2 minutes)\n",
        "\n",
        "### Checklist when deploying:\n",
        "0. Run the script from Colab, jupyter notebook, or another environment that can support more than 1 hour of run time using `fetch_method = \"full\"`. You can now move it to a microservice with a shorter timeout.\n",
        "1. Set the environmental veriables `client_id` and `client_secret`. These can be found by logging into ilevel. (https://clients.ilevelsolutions.com/)\n",
        "2. Delete one of the first code cells called `Running in Colabs (to be deleted when running in Cloud functions)``\n",
        "3. Delete the very last line `main()`, as this is called by Cloud Function by \"entry point\"\n",
        "4. Change `fetch_method = \"full\"` to `fetch_method = \"append\"`, or your script will most likely timeout.\n",
        "5. Change `bq_dataset_name = \"fsn_insight_test\"` to `bq_dataset_name = \"fsn_insight_ldg\"`, if you want to write to the prod tables.\n",
        "\n",
        "### iLevel API\n",
        "\n",
        "iLevel API documentation (which is surprisingly good) can be found here:\n",
        "[https://sand-docs.ilevelsolutions.eu/](https://sand-docs.ilevelsolutions.eu/)\n",
        "\n",
        "The script is fairly straight forward, except for two quirks:\n",
        "1. The biggest table `periodicData` has a limit of only 1000 results per page, and a maximum of 100 pages (=100,000 records). To get around this (we currently have 2,000,000 records in this table), we keep adding filters to this table until we can chunk it to chunks of < 100,000 per chunk. This is described in detail further down.\n",
        "2. Some endpoints support pagination, some dont. Some support sorting, some dont. Some have a date field, some have a datetime field, and some have neither. It's a bit of a mess. And the API is extremely picky on what it will accept. The block `main` handles each supported feature per endpoint.\n",
        "\n",
        "Because of the behavior described in (2) above, we do the append function like this:\n",
        "- For the `periodicData` table, we fetch only the records that was added after the latest push.\n",
        "- For all the rest, we check if the current amount of items in biquery is equal to what is served by the API endpoint. If it is, we ignore and move on, if it's not, we do a full refresh.\n",
        "- Oh, why don't we do a purge of the tables before we do a full refresh of mostly static data I hear you ask? I'll tell you why: BigQuery panics when you do a purge+write. And you end up with an error message that tells you the data stream is busy. This is super hard to replicate consistantly, and testing shows that sometimes you need to wait 10+ minutes after a purge before you can write to the table. So, we ignore, and just fill the tables with some duplicates. This is mostly static data, so most of the times nothing is written anyway...\n",
        "\n",
        "### Final words\n",
        "\n",
        "Generate `requirements.txt` by running `pip freeze` in a cell.<br>\n",
        "```\n",
        "google-cloud-bigquery==1.21.0\n",
        "requests==2.23.0\n",
        "urllib3==1.23 <= Script crashed with \"SSL ERROR\" with the latest release (known bug)\n",
        "```\n",
        "<br>\n",
        "\n",
        "To trigger the live script, use:\n",
        "```\n",
        "gcloud pubsub topics publish trigger_ilevel-to-bigquery --message=None\n",
        "```\n",
        "\n",
        "Also, just to be sure, the link to the Colab page for this script is:\n",
        "[https://colab.research.google.com/drive/1Uh3szpV7JtxUEsl2jY3b-YHa1ZxxU6yM?usp=sharing](https://colab.research.google.com/drive/1Uh3szpV7JtxUEsl2jY3b-YHa1ZxxU6yM?usp=sharing)\n",
        "\n",
        "-Chris (cc@fsncapital.com)"
      ],
      "metadata": {
        "id": "Iml645O7Zimn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNSiY1T93ONb"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ0D5p2I1OJj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import sys\n",
        "import timeit\n",
        "import calendar\n",
        "import urllib.parse\n",
        "import logging\n",
        "from abc import ABC, abstractmethod\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import urlparse\n",
        "from urllib.parse import parse_qs\n",
        "from abc import ABC, abstractmethod\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "from google.cloud import secretmanager\n",
        "from google.colab import auth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnxDIQ1d3RCc"
      },
      "source": [
        "## Running in Colabs (to be deleted when running in Cloud functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23aZELE01OJm"
      },
      "outputs": [],
      "source": [
        "# TO BE DELETED WHEN RUNNING IN THE CLOUD\n",
        "\n",
        "# GET SECRETS\n",
        "!pip install google-cloud-secret-manager\n",
        "\n",
        "from google.cloud import secretmanager\n",
        "from google.oauth2 import service_account\n",
        "from google.colab import auth\n",
        "\n",
        "credentials = service_account.Credentials.from_service_account_file(\n",
        "   r'/content/drive/MyDrive/Colab Notebooks/fsn-insight-b64b318df8c7.json')\n",
        "\n",
        "secret_client = secretmanager.SecretManagerServiceClient(credentials=credentials)\n",
        "project_id = 'fsn-insight' \n",
        "\n",
        "secret_name = \"ilevel_client_id\" \n",
        "resource_name = f\"projects/{project_id}/secrets/{secret_name}/versions/latest\" \n",
        "response = secret_client.access_secret_version(request={\"name\": resource_name})\n",
        "os.environ[\"client_id\"] = response.payload.data.decode('UTF-8')\n",
        "\n",
        "secret_name = \"ilevel_client_secret\" \n",
        "resource_name = f\"projects/{project_id}/secrets/{secret_name}/versions/latest\" \n",
        "response = secret_client.access_secret_version(request={\"name\": resource_name})\n",
        "os.environ[\"client_secret\"] = response.payload.data.decode('UTF-8')\n",
        "\n",
        "# Set other variables\n",
        "os.environ[\"runtime\"] = \"local\" # \"gcp\" for gcp\n",
        "\n",
        "# Authenticate\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoPeSny5o6y0"
      },
      "source": [
        "## Set variables and declare functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6lBveSn1OJn",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Set environmental variables\n",
        "start = timeit.default_timer()\n",
        "runtime = os.environ.get(\"runtime\")\n",
        "url_base = \"https://api.ilevelsolutions.com/v1/\"\n",
        "fetched_at = datetime.utcnow().isoformat()[:-3]+'Z'\n",
        "fetched_by = \"ilevel-to-bigquery script on GCP Cloud Functions\"\n",
        "fetch_method = \"full\" # Extracts all data. Takes more than 1 hour to run\n",
        "#fetch_method = \"append\" # Truncates all tables and extracts all data, except for periodicData, where it does an append. Takes < 10 minutes to run\n",
        "access_token = {}\n",
        "total_count = 0\n",
        "bq_project_name = \"fsn-insight\"\n",
        "#bq_dataset_name = \"fsn_insight_ldg\" # prod dataset\n",
        "bq_dataset_name = \"fsn_insight_test\" # testing dataset\n",
        "bq_client = bigquery.Client(bq_project_name)\n",
        "read_buffer_size = 10000\n",
        "ilevel_page_size = 1000 # Pagination limit by ilevel\n",
        "\n",
        "# Get secrets\n",
        "credentials = service_account.Credentials.from_service_account_file(\n",
        "   r'/content/drive/MyDrive/Colab Notebooks/fsn-insight-b64b318df8c7.json')\n",
        "\n",
        "secret_client = secretmanager.SecretManagerServiceClient(credentials=credentials)\n",
        "project_id = 'fsn-insight' \n",
        "\n",
        "secret_name = \"ilevel_client_id\" \n",
        "resource_name = f\"projects/{project_id}/secrets/{secret_name}/versions/latest\" \n",
        "response = secret_client.access_secret_version(request={\"name\": resource_name})\n",
        "os.environ[\"client_id\"] = response.payload.data.decode('UTF-8')\n",
        "\n",
        "secret_name = \"ilevel_client_secret\" \n",
        "resource_name = f\"projects/{project_id}/secrets/{secret_name}/versions/latest\" \n",
        "response = secret_client.access_secret_version(request={\"name\": resource_name})\n",
        "os.environ[\"client_secret\"] = response.payload.data.decode('UTF-8')\n",
        "\n",
        "\n",
        "# Helpers for each API endpoint for fetching data\n",
        "read_buffer = []\n",
        "\n",
        "# Helpers for filtering periodicData\n",
        "list_assets = []\n",
        "list_scenarios = []\n",
        "list_dataItems = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6bN09VIU1dI"
      },
      "source": [
        "## Shared functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_access_token()"
      ],
      "metadata": {
        "id": "H7mMZAPymedK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EigNofRy1OJo"
      },
      "outputs": [],
      "source": [
        "# Get access token\n",
        "def get_access_token():\n",
        "    global access_token\n",
        "    if(\"expires_at\" in access_token and access_token[\"expires_at\"] > datetime.now() + timedelta(seconds = 30)): # adding some seconds in margin\n",
        "        return access_token\n",
        "    try:\n",
        "        r = requests.post(\n",
        "            url_base + \"token\",\n",
        "            headers={\"Accept\": \"application/json\",\n",
        "                \"Content-Type\":\"application/x-www-form-urlencoded\"},\n",
        "            data={\"grant_type\": \"client_credentials\"},\n",
        "            auth=(client_id, client_secret)\n",
        "        )\n",
        "        access_token = r.json()[\"access_token\"]\n",
        "        expires_in = r.json()[\"expires_in\"]\n",
        "        expires_at = datetime.now() + timedelta(seconds = expires_in)\n",
        "        access_token = {\"access_token\":access_token, \"expires_in\":expires_in, \"expires_at\":expires_at}\n",
        "        return access_token\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error while refreshing access token: {e}\") from e  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_next_page()"
      ],
      "metadata": {
        "id": "phZ4XA7Fmin2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kHA0vcd1OJp"
      },
      "outputs": [],
      "source": [
        "# Check if more pages and return page number if there is a next page\n",
        "def get_next_page(r):\n",
        "    try:\n",
        "        if r.json()[\"links\"].get(\"next\"):\n",
        "            url_next = r.json()[\"links\"][\"next\"]\n",
        "            next_page_number = int(parse_qs(urlparse(url_next).query)['page[number]'][0])\n",
        "            return {\"page[number]\" : str(next_page_number)}\n",
        "    except:\n",
        "        return None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### timer()"
      ],
      "metadata": {
        "id": "Ws03A4HAmslC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVcxUiSRI8TY"
      },
      "outputs": [],
      "source": [
        "# Get a string with the current running time\n",
        "def timer():\n",
        "    stop = timeit.default_timer()\n",
        "    total_time = stop - start\n",
        "\n",
        "    # output running time in a nice format.\n",
        "    mins, secs = divmod(total_time, 60)\n",
        "    hours, mins = divmod(mins, 60)\n",
        "    \n",
        "    hours, rem = divmod(stop-start, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "\n",
        "    return \"{:0>2}:{:0>2}:{:0>2}\".format(int(hours),int(minutes),int(seconds))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### log()"
      ],
      "metadata": {
        "id": "fasnHJyKmt7y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUdkfo4FS0J8"
      },
      "outputs": [],
      "source": [
        "# Log function. To be updated with Stackdriver\n",
        "def log(msg):\n",
        "    print(timer() + \" \" + str(msg)) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load_data_to_bigquery()"
      ],
      "metadata": {
        "id": "7lpWXZBNmvzG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9kMUZiT1OJq"
      },
      "outputs": [],
      "source": [
        "# Post data to BigQuery\n",
        "def load_data_to_bigquery(table, records, truncate=False):\n",
        "    bq_table_full_path = f\"\"\"{bq_project_name}.{bq_dataset_name}.{table}\"\"\"\n",
        "    rows_to_insert = records\n",
        "    global total_count\n",
        "\n",
        "    try:\n",
        "        errors = bq_client.insert_rows_json(bq_table_full_path, rows_to_insert)\n",
        "        total_count += len(rows_to_insert)\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to post to BQ: {e}\") from e\n",
        "    \n",
        "    if errors == []:\n",
        "        log(\"Loaded \" + str(len(rows_to_insert)) + \" rows into \" + bq_table_full_path + \" (\" + f'{total_count:,}' + \" in total)\" )\n",
        "    else:\n",
        "        log(\"Encountered errors while inserting rows to BigQuery: {}\".format(errors))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### bq_get_number_of_latest_records()"
      ],
      "metadata": {
        "id": "M4u5IXg4DXwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bq_get_number_of_latest_records(table):\n",
        "    bq_table_full_path = f\"\"\"{bq_project_name}.{bq_dataset_name}.{table}\"\"\"\n",
        "    latest_fetch_in_bq = \"\"\n",
        "\n",
        "    try:\n",
        "        # Find the latest load date time from BigQuery\n",
        "        latest_fetch_in_bq = get_last_upload(table)\n",
        "\n",
        "        # Count how many records where loaded in the last run\n",
        "        query = \"SELECT COUNT(fetched_at) as number_of_records FROM `\" + str(bq_table_full_path) + \"` WHERE fetched_at = \\\"\" + str(latest_fetch_in_bq) + \"\\\"\"\n",
        "        result = bq_client.query(query)\n",
        "        rows = result.result()\n",
        "        for row in rows:\n",
        "            return row.number_of_records\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to query BQ: {e}\") from e"
      ],
      "metadata": {
        "id": "6xrESBkBDbgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_last_upload"
      ],
      "metadata": {
        "id": "s6KPtrzEm4mG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_last_upload(table):\n",
        "    client = bigquery.Client(project=bq_project_name)\n",
        "    query = \"SELECT fetched_at FROM \" + bq_project_name + \".\" + bq_dataset_name + \".\" + table + \" ORDER BY fetched_at DESC LIMIT 1\"\n",
        "    try:\n",
        "        result = client.query(query)  # Make an API request.\n",
        "        for row in result:\n",
        "            return row[0]\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "4JxUhfV-B8jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bH3VSZiBIQW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Head"
      ],
      "metadata": {
        "id": "qoKanOKHRKH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def head():\n",
        "    log(\"Starting extract/load iLevel/bigquery\")\n",
        "    log(\"Time (UTC): \" + fetched_at)\n",
        "    log(\"Fetch method: \" + fetch_method)"
      ],
      "metadata": {
        "id": "tXsd7zt_RM_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJdm2sQmA46A"
      },
      "source": [
        "## periodicData filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r5hOylDBJf0"
      },
      "source": [
        "### Explanation of filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63SaA43h1OJq"
      },
      "source": [
        "\n",
        "https://sand-docs.ilevelsolutions.eu/<br/>\n",
        "Get list of all filter items to be used for periodicdata. As iLevel has a limit of 100 pages with 1000 records each, we need to divide the query. We built three lists: assets, scenarios, and data items. We then build one master list of filters covering the dataset. If we could sort and filter by ID (we cannot) we could just read as far as we came, filter, and repeat. But since each query can only contain max 100,000 records, we need to get below this number at the filter level on the query.\n",
        "\n",
        "It is worth noting that this limiation only applies to the periodicData endpoint.\n",
        "\n",
        "Tested on a dataset of ~2.000.000 records, we found ~50 assets ~10 scenarios and ~2.000 dataItems: \n",
        "- Downloaded entire dataset using ~2.200 querries and ~1 hour \n",
        "- The three filters plus splitting on years or months, we are comfortably within iLevel's limitations.\n",
        "\n",
        "\n",
        "### The way this script works: \n",
        "It starts extracting all data for the first investment. \n",
        " 1. If it returns less than 100,000 records, it will go to the next investment in the list_assets list. \n",
        " 2. If it returns more than 100,000 records, it keep adding more filters until it hits less than 100,000.\n",
        "<br>The order of filters are `[investment]` -> `[scenarios]` -> `[periodEnd (Year then month)]` -> `[dataItem]`\n",
        "<br>Which can look like this `[Acme Inc]` -> `[Budget]` -> `[ge(2020-01-01),le(2020-12-31)]` -> `[Revenue]`\n",
        "\n",
        "#### Example of list of filters\n",
        "For three companies named \"**Small**\", \"**Medium**\", and \"**Large**\", the `filter_params` list could look something like this:\n",
        "\n",
        "\n",
        "*Entire asset \"Small\" fits in 100k records. Sweet!*\n",
        "\n",
        "```\n",
        "filter_params[0]  {\"page[size]\":\"1000\", \"filter[investment]\":\"Small\"}`\n",
        "```\n",
        "\n",
        "*Asset \"Medium\" needs to be split into scenarios to fit in 100k records*\n",
        "\n",
        "```\n",
        "filter_params[1]  {\"page[size]\":\"1000\", \"filter[investment]\":\"Medium\", \"filter[scenario]\":\"Actual\"}\n",
        "filter_params[2]  {\"page[size]\":\"1000\", \"filter[investment]\":\"Medium\", \"filter[scenario]\":\"Budget\"}\n",
        "filter_params[3]  {\"page[size]\":\"1000\", \"filter[investment]\":\"Large\", \"filter[scenario]\":\"Actiual\"}\n",
        "```\n",
        "\n",
        "*`Actual` is less than 100k, but `Budget` is not. Budget needs to be split into years*\n",
        "\n",
        "```\n",
        "filter_params[4]  {\"page[size]\":\"1000\", \"filter[investment]\":\"Large\", \"filter[scenario]\":\"Budget\", \"filter[periodEnd]\":\"[ge(2020-01-01),le(2020-12-31)]\"}\n",
        "```\n",
        "\n",
        "*`Budget` for `2020` is less than 100k records, but `2021` has more, which is therefore split by month*\n",
        "\n",
        "```\n",
        "filter_params[5]  {\"page[size]\":\"1000\", \"filter[investment]\":\"Large\", \"filter[scenario]\":\"Budget\", \"filter[periodEnd]\":\"[ge(2021-01-01),le(2021-01-31)]\"}\n",
        "```\n",
        "\n",
        "*Budget for `Jan 2021` is less than 100k records, and so are each month...*<br> \n",
        "```\n",
        "filter_params[6]  {\"page[size]\":\"1000\", \"filter[investment]\":\"Large\", \"filter[scenario]\":\"Budget\", \"filter[periodEnd]\":\"[ge(2021-02-01),le(2021-02-31)]\"}filter_params[7]  {\"page[size]\":\"1000\", \"filter[investment]\":\"Large\", \"filter[scenario]\":\"Budget\", \"filter[periodEnd]\":\"[ge(2021-03-01),le(2021-03-31)]\"}\n",
        "filter_params[8]  {\"page[size]\":\"1000\", \"filter[investment]\":\"Large\", \"filter[scenario]\":\"Budget\", \"filter[periodEnd]\":\"[ge(2021-04-01),le(2021-04-31)]\"}\n",
        "filter_params[9]  {\"page[size]\":\"1000\", \"filter[investment]\":\"Large\", \"filter[scenario]\":\"Budget\", \"filter[periodEnd]\":\"[ge(2021-05-01),le(2021-05-31)]\"}\n",
        "filter_params[10] {\"page[size]\":\"1000\", \"filter[investment]\":\"Large\", \"filter[scenario]\":\"Budget\", \"filter[periodEnd]\":\"[ge(2021-06-01),le(2021-06-31)]\"} \n",
        "```\n",
        "*...except for `July`, which is broken down into `dataItems` to stay below 100k* <br> \n",
        "\n",
        "```\n",
        "filter_params[11] {\"page[size]\":\"1000\", \"filter[investment]\":\"Large\", \"filter[scenario]\":\"Budget\", \"filter[periodEnd]\":\"[ge(2021-07-01),le(2021-07-31)]\", \"filter[dataItem]\":\"Revenue\" }\n",
        "filter_params[12] {\"page[size]\":\"1000\", \"filter[investment]\":\"Large\", \"filter[scenario]\":\"Budget\", \"filter[periodEnd]\":\"[ge(2021-07-01),le(2021-07-31)]\", \"filter[dataItem]\":\"EBITDA\" }\n",
        "filter_params[13] {\"page[size]\":\"1000\", \"filter[investment]\":\"Large\", \"filter[scenario]\":\"Budget\", \"filter[periodEnd]\":\"[ge(2021-08-01),le(2021-08-31)]\"}\n",
        "filter_params[14] {\"page[size]\":\"1000\", \"filter[investment]\":\"Large\", \"filter[scenario]\":\"Budget\", \"filter[periodEnd]\":\"[ge(2021-09-01),le(2021-09-31)]\"}\n",
        "filter_params[15] {\"page[size]\":\"1000\", \"filter[investment]\":\"Large\", \"filter[scenario]\":\"Budget\", \"filter[periodEnd]\":\"[ge(2021-10-01),le(2021-10-31)]\"}\n",
        "filter_params[16] {\"page[size]\":\"1000\", \"filter[investment]\":\"Large\", \"filter[scenario]\":\"Budget\", \"filter[periodEnd]\":\"[ge(2021-11-01),le(2021-11-31)]\"}\n",
        "filter_params[17] {\"page[size]\":\"1000\", \"filter[investment]\":\"Large\", \"filter[scenario]\":\"Budget\", \"filter[periodEnd]\":\"[ge(2021-12-01),le(2021-12-31)]\"}\n",
        "```\n",
        "\n",
        "By breaking it down like above, we ensure that we minimize the amount of querries needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4OBckTGBUHG"
      },
      "source": [
        "### Build list of assets, scenarios, and dataItems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTKv2Nhu1OJs",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Build list of available Assets, Scenarios, and DataItems from iLevel\n",
        "def build_lists_of_assets_scenarios_dataitems():\n",
        "\n",
        "    headers = {\"Authorization\": \"Bearer \" + get_access_token()[\"access_token\"]}\n",
        "    params = {}\n",
        "\n",
        "    log(\"Starting to extract PeriodicData. As this API endpoint has a limit of 100 pages from the iLevel API, we need to build lists of filters, using Assets as the root filter.\")\n",
        "\n",
        "    # Build list of all Assets\n",
        "    log(\"Building list of all assets\")\n",
        "    url_self = url_base + \"entities/assets\"\n",
        "    params = {\"page[size]\":\"1000\", \"page[number]\":\"1\", \"fields[assets]\":\"\\\"name\\\"\", \"sort\":\"name\"}\n",
        "    try:\n",
        "        while True:\n",
        "            # Loop all pages - if any\n",
        "            r = requests.get(url=url_self, headers=headers, params=urllib.parse.urlencode(params, safe='[]'))\n",
        "            for x in r.json()[\"data\"]:\n",
        "                list_assets.append(x[\"attributes\"][\"name\"])\n",
        "            next_page = get_next_page(r)\n",
        "            if(not next_page): break\n",
        "            params.update(get_next_page(r))\n",
        "            \n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error while getting list of asset names: {e}\") from e  \n",
        "\n",
        "\n",
        "    # Build list of all Scenarios\n",
        "    log(\"Building list of all scenarios\")\n",
        "    headers = {\"Authorization\": \"Bearer \" + get_access_token()[\"access_token\"]}\n",
        "\n",
        "    try:\n",
        "        url_self = url_base + \"scenarios\"  \n",
        "        r = requests.get(url=url_self, headers=headers)\n",
        "        for x in r.json()[\"data\"]:\n",
        "            list_scenarios.append(x[\"attributes\"][\"name\"])\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error while getting list of scenario names: {e}\") from e\n",
        "        \n",
        "\n",
        "    # Build list of dataItems\n",
        "    log(\"Building list of all dataItems\")\n",
        "    headers = {\"Authorization\": \"Bearer \" + get_access_token()[\"access_token\"]}\n",
        "    url_self = url_base + \"dataItems\"\n",
        "    params = {\"page[size]\":\"1000\", \"page[number]\":\"1\", \"fields[dataItems]\":\"\\\"name\\\"\", \"sort\":\"name\"}\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            # Loop all pages - if any\n",
        "            r = requests.get(url=url_self, headers=headers, params=urllib.parse.urlencode(params, safe='[]'))\n",
        "            for x in r.json()[\"data\"]:\n",
        "                list_dataItems.append(x[\"attributes\"][\"name\"])\n",
        "            next_page = get_next_page(r)\n",
        "            if(not next_page): break\n",
        "            params.update(get_next_page(r))\n",
        "                \n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error while getting list of dataItems names: {e}\") from e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4VvZJ8v1OJt"
      },
      "source": [
        "### Build master list of parameters for extracting data from periodicData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzFX576e1OJt",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "def get_filter_list(list_assets, list_scenarios, list_dataItems):\n",
        "    # Start building master list to filter_params\n",
        "\n",
        "    filter_params = []\n",
        "    subtotal_count = 0\n",
        "    params = {}\n",
        "\n",
        "    start_time = get_last_upload(\"ilevel_periodicData_ldg\")\n",
        "\n",
        "    if(fetch_method==\"append\"):\n",
        "        log(\"Fetching data modified after: \" + start_time)\n",
        "    log(\"Building master list of all filters\")\n",
        "    \n",
        "    try:\n",
        "        headers = {\"Authorization\": \"Bearer \" + get_access_token()[\"access_token\"]}\n",
        "        url_self = url_base + \"periodicData\"\n",
        "        params = {\"page[size]\":\"1\", \"page[number]\":\"1\", \"sort\":\"lastModifiedDate\"}\n",
        "        r = requests.get(url=url_self, headers=headers, params=params)\n",
        "        count = r.json()[\"meta\"][\"count\"]\n",
        "\n",
        "        if(count <= 100000):\n",
        "            params.update({\"page[size]\":\"1000\", \"page[number]\":\"1\"})\n",
        "            verbose = \"(\" + f'{count:,}' + \")\"\n",
        "            filter_params.append({\"params\":params,\"count\":count,\"url\":r.url,\"verbose\":verbose})\n",
        "            log(\"Added (\" + f'{count:,}' + \") to list of filters\")\n",
        "            subtotal_count += count\n",
        "\n",
        "        elif(count > 100000): \n",
        "            log(\"Periodic data has more than 100,000 items. (\" + f'{count:,}' + \" found) Splitting query into assets.\")\n",
        "\n",
        "            for a in list_assets:\n",
        "                # Reset params for each loop\n",
        "                params = {\"page[size]\":\"1\", \"page[number]\":\"1\", \"sort\":\"lastModifiedDate\", \"filter[investment]\":\"\\\"\" + a + \"\\\"\"}\n",
        "                if(start_time and fetch_method == \"append\"):\n",
        "                    params.update({\"filter[lastModifiedDate]\":\"ge(\" + start_time + \")\"})\n",
        "\n",
        "                # Check to see if more than 100,000 records with filter \"investment\"\n",
        "                headers = {\"Authorization\": \"Bearer \" + get_access_token()[\"access_token\"]}\n",
        "                r = requests.get(url=url_self, headers=headers, params=params)\n",
        "                count = r.json()[\"meta\"][\"count\"]\n",
        "                \n",
        "                if(count == 0):\n",
        "                    continue\n",
        "                \n",
        "                # If less than 100,000 records, append the filter to the list\n",
        "                if(count <= 100000):\n",
        "                    params.update({\"page[size]\":\"1000\", \"page[number]\":\"1\"})\n",
        "                    verbose = \"[\" + a + \"] (\" + f'{count:,}' + \")\"\n",
        "                    filter_params.append({\"params\":params,\"count\":count,\"url\":r.url,\"verbose\":verbose})\n",
        "                    log(\"Added [\" + a + \"] (\" + f'{count:,}' + \") to list of filters\")\n",
        "                    subtotal_count += count\n",
        "                \n",
        "                # If more than 100,000 records, add another filter \"scenario\"\n",
        "                else:\n",
        "                    log(\"Periodic data for [\" + a + \"] has more than 100,000 items. (\" + f'{count:,}' + \" found) Splitting query into scenarios.\")\n",
        "                    for b in list_scenarios:\n",
        "                        # Reset params for each loop\n",
        "                        params = {\"page[size]\":\"1\", \"page[number]\":\"1\", \"sort\":\"lastModifiedDate,scenario\", \"filter[investment]\":\"\\\"\" + a + \"\\\"\"}\n",
        "                        if(start_time and fetch_method == \"append\"):\n",
        "                            params.update({\"filter[lastModifiedDate]\":\"ge(\" + start_time + \")\"})\n",
        "\n",
        "                        # Add filter \"investment\" and \"scenario\" and check to see if we get more than 100,000 records with this filter \n",
        "                        headers = {\"Authorization\": \"Bearer \" + get_access_token()[\"access_token\"]}\n",
        "                        params.update({\"page[size]\":\"1\", \"filter[scenario]\":\"\\\"\" + b + \"\\\"\"})\n",
        "                        r = requests.get(url=url_self, headers=headers, params=params)\n",
        "                        count = r.json()[\"meta\"][\"count\"]\n",
        "\n",
        "                        if(count == 0):\n",
        "                            continue\n",
        "                            \n",
        "                        # If less than 100,000 records, append the filter to the list\n",
        "                        if(count <= 100000):\n",
        "                            params.update({\"page[size]\":\"1000\", \"page[number]\":\"1\"})\n",
        "                            verbose = \"[\" + a + \"] [\" + b + \"] (\" + f'{count:,}' + \")\"\n",
        "                            filter_params.append({\"params\":params,\"count\":count,\"url\":r.url,\"verbose\":verbose})\n",
        "                            log(\"Added [\" + a + \"] [\" + b + \"] (\" + f'{count:,}' + \") to list of filters\")\n",
        "                            subtotal_count += count\n",
        "\n",
        "                        # If more than 100,000 records, add another filter \"year\" (going one year at a time)\n",
        "                        else:\n",
        "                            # Find first date with data\n",
        "                            headers = {\"Authorization\": \"Bearer \" + get_access_token()[\"access_token\"]}\n",
        "                            params.update({\"page[size]\":\"1\", \"sort\":\"periodEnd\"})\n",
        "                            r = requests.get(url=url_self, headers=headers, params=params)\n",
        "                            first_date = r.json()[\"data\"][0][\"attributes\"][\"periodEnd\"]\n",
        "\n",
        "                            # Find last date with data\n",
        "                            params.update({\"sort\":\"-periodEnd\"})\n",
        "                            r = requests.get(url=url_self, headers=headers, params=params)\n",
        "                            last_date = r.json()[\"data\"][0][\"attributes\"][\"periodEnd\"]\n",
        "\n",
        "                            first_year = int(first_date[0:4])\n",
        "                            last_year = int(last_date[0:4])\n",
        "\n",
        "                            log(\"Periodic data for [\" + a + \"] [\" + b + \"] has more than 100,000 items. (\" + f'{count:,}' + \" found) Splitting query into years. (\" + str(first_year) + \"-\" + str(last_year) + \")\")\n",
        "\n",
        "                            for y in range(first_year, last_year+1):\n",
        "                                # Reset params for each loop\n",
        "                                headers = {\"Authorization\": \"Bearer \" + get_access_token()[\"access_token\"]}\n",
        "                                params = {\"page[size]\":\"1\", \"page[number]\":\"1\", \"filter[investment]\":\"\\\"\" + a + \"\\\"\", \"filter[scenario]\":\"\\\"\" + b + \"\\\"\"}\n",
        "                                \n",
        "                                if(start_time and fetch_method == \"append\"):\n",
        "                                    params.update({\"filter[lastModifiedDate]\":\"ge(\" + start_time + \")\"})\n",
        "                                \n",
        "                                params.update({\"filter[periodEnd]\":[\"ge(\" + str(y) + \"-01-01)\", \"le(\" + str(y) + \"-12-31)\"]})\n",
        "                                \n",
        "                                # Count records in the year\n",
        "                                r = requests.get(url=url_self, headers=headers, params=params)\n",
        "                                count = r.json()[\"meta\"][\"count\"]\n",
        "\n",
        "                                if(count == 0):\n",
        "                                    continue\n",
        "                                \n",
        "                                # If year has less than 100,000 records, append the filter to the list\n",
        "                                if(count <= 100000):\n",
        "                                    params.update({\"page[size]\":\"1000\"})\n",
        "                                    verbose = \"[\" + a + \"] [\" + b + \"] [\" + str(y) + \"] (\" + f'{count:,}' + \")\"\n",
        "                                    filter_params.append({\"params\":params,\"count\":count,\"url\":r.url,\"verbose\":verbose})\n",
        "                                    log(\"Added [\" + a + \"] [\" + b + \"] [\" + str(y) + \"] (\" + f'{count:,}' + \") to list of filters\")\n",
        "                                    subtotal_count += count\n",
        "\n",
        "                                # If more than 100,000 records, add another filter \"month\" (going one month at a time)\n",
        "                                else:\n",
        "                                    log(\"Periodic data for [\" + a + \"] [\" + b + \"] [\" + str(y) + \"] has more than 100,000 items. (\" + f'{count:,}' + \" found) Splitting query into months.\")\n",
        "                                    for m in range(1,13):\n",
        "                                        # Reset params for each loop\n",
        "                                        headers = {\"Authorization\": \"Bearer \" + get_access_token()[\"access_token\"]}\n",
        "                                        params = {\"page[size]\":\"1\", \"page[number]\":\"1\", \"filter[investment]\":\"\\\"\" + a + \"\\\"\", \"filter[scenario]\":\"\\\"\" + b + \"\\\"\"}\n",
        "                                        if(start_time and fetch_method == \"append\"):\n",
        "                                            params.update({\"filter[lastModifiedDate]\":\"ge(\" + start_time + \")\"})\n",
        "\n",
        "                                        # Update with filter per month and count records for each month\n",
        "                                        params.update({\"filter[periodEnd]\":[\"ge(\" + str(y) + \"-\" + str(m).zfill(2) + \"-01)\", \n",
        "                                            \"le(\" + str(y) + \"-\" + str(m).zfill(2) + \"-\" + str(calendar.monthrange(y, m)[1]) + \")\"]})\n",
        "                                        r = requests.get(url=url_self, headers=headers, params=params)\n",
        "                                        count = r.json()[\"meta\"][\"count\"]\n",
        "\n",
        "                                        if(count == 0):\n",
        "                                            continue\n",
        "                                        \n",
        "                                        # If less than 100,000 records, append the filter to the list\n",
        "                                        if(count <= 100000):\n",
        "                                            params.update({\"page[size]\":\"1000\"})\n",
        "                                            verbose = \"[\" + a + \"] [\" + b + \"] [\" + str(y) + \"] [\" + str(m).zfill(2) + \"] (\" + f'{count:,}' + \")\"\n",
        "                                            filter_params.append({\"params\":params,\"count\":count,\"url\":r.url,\"verbose\":verbose})\n",
        "                                            log(\"Added [\" + a + \"] [\" + b + \"] [\" + str(y) + \"] [\" + str(m).zfill(2) + \"] (\" + f'{count:,}' + \") to list of filters\")\n",
        "                                            subtotal_count += count\n",
        "\n",
        "                                        # If more than 100,000 records, ad the dataItems filter\n",
        "                                        else:\n",
        "                                            log(\"Periodic data for [\" + a + \"] [\" + b + \"] [\" + str(y) + \"] [\" + str(m) + \"] has more than 100,000 items. (\" + f'{count:,}' + \" found) Splitting query into data items.\")\n",
        "                                            for c in list_dataItems:\n",
        "                                                # Reset params for each loop\n",
        "                                                params = {\"page[size]\":\"1\", \"page[number]\":\"1\", \"filter[investment]\":\"\\\"\" + a + \"\\\"\", \"filter[scenario]\":\"\\\"\" + b + \"\\\"\"}\n",
        "                                                params.update({\"page)[size]\":\"1\",\"filter[periodEnd]\":[\"ge(\" + str(y) + \"-\" + str(m).zfill(2) + \"-01)\", \n",
        "                                                                \"le(\" + str(y) + \"-\" + str(m).zfill(2) + \"-\" + str(calendar.monthrange(y, m)[1]) + \")\"]})\n",
        "                                                if(start_time and fetch_method == \"append\"):\n",
        "                                                    params.update({\"filter[lastModifiedDate]\":\"ge(\" + start_time + \")\"})\n",
        "\n",
        "                                                # Update with filter per data item\n",
        "                                                params.update({\"filter[dataItem]\":\"\\\"\" + c + \"\\\"\"})\n",
        "                                                verbose = \"[\" + a + \"] [\" + b + \"] [\" + str(y) + \"] [\" + str(m).zfill(2) + \"] [\" + c + \"] (\" + f'{count:,}' + \")\"\n",
        "                                                filter_params.append({\"params\":params,\"count\":None,\"url\":None,\"verbose\":verbose})\n",
        "                                                log(\"Added [\" + a + \"] [\" + b + \"] [\" + str(y) + \"] [\" + str(m).zfill(2) + \"] [\" + c + \"] (\" + f'{count:,}' + \") to list of filters\")\n",
        "                                                subtotal_count += count\n",
        "\n",
        "                                                # If you have more than 100,000 elements, when you filter on \"investment\", \"scenario\", \"year\", \n",
        "                                                # \"month\", and \"dataItem\", I can't do anything more for you. You need to fix your broken iLevel implementation.\n",
        "                                                # As data items can be a very long list, the script will not make a request and count each one, instead it\n",
        "                                                # will run it blind, assuming none fo the data items returns more than 100,000 records.\n",
        "                                                # If it does return more than 100,000 records, the script will just cut off, and continue like nothing\n",
        "                                                # happened, but you will be missing the data in your target.\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Last request: \" + str(r) + \" \" + r.url + \" \" + str(r.json()) + \"{e}\") from e\n",
        "    \n",
        "    log(\"Finished splitting data in chunks of < 100,000. Total items found: \" + f'{subtotal_count:,}')\n",
        "    \n",
        "    return filter_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gryKuer-oA3q"
      },
      "source": [
        "## Define superclass for endpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aEl_uv_XG1r"
      },
      "outputs": [],
      "source": [
        "class ilevel_to_bigquery(ABC):\n",
        "    def __init__(self, endpoint, sort = None, append_supported = False, pagination_supported = True, filters=None,deleted_data=False):\n",
        "        self.endpoint = endpoint\n",
        "        self.bigquery_table = \"ilevel_\" + endpoint.rsplit('/', 1)[-1] + \"_ldg\"\n",
        "        self.sort = sort\n",
        "        self.append_supported = append_supported\n",
        "        self.pagination_supported = pagination_supported\n",
        "        self.read_buffer = []\n",
        "        self.subtotal_count = 0\n",
        "        self.url_self = url_base + self.endpoint\n",
        "        self.deleted_data = deleted_data\n",
        "        self.start_time = get_last_upload(self.bigquery_table)\n",
        "        self.params = {}\n",
        "        self.headers = {}\n",
        "        self.fetched_by = fetched_by\n",
        "        self.fetched_at = fetched_at\n",
        "        self.fetch_method = fetch_method\n",
        "        self.truncate = False\n",
        "        \n",
        "        # If list of filters exist (used for extracting periodicData)\n",
        "        if(filters):\n",
        "            i = 1\n",
        "            for f in filters:\n",
        "                v = str(f[\"verbose\"]) + \" - Progress: \" + str(i) + \"/\" + str(len(filters))\n",
        "                self.extract_data_from_ilevel_to_bigquery(params=f[\"params\"],verbose=v)\n",
        "                i += 1\n",
        "        else:\n",
        "            self.extract_data_from_ilevel_to_bigquery()\n",
        "    \n",
        "    def extract_data_from_ilevel_to_bigquery(self, params=None, verbose=None):\n",
        "        # Extract data from ilevel, and loads it into bigquery\n",
        "\n",
        "        # Skip if appending data, and the table is not part of the append\n",
        "        # (For not duplicating massive amounts of data that rarely changes)\n",
        "        \n",
        "        s = \"Extracting data from \" + self.endpoint\n",
        "        if(verbose):\n",
        "            s += \" \" + str(verbose)\n",
        "        log(s)\n",
        "\n",
        "        # Add fetch method to query\n",
        "        if(not self.append_supported):\n",
        "            self.fetch_method = \"full\"\n",
        "            self.truncate = True\n",
        "            \n",
        "        \n",
        "        # Build params\n",
        "        self.params = {}\n",
        "\n",
        "        if(params):\n",
        "            self.params.update(params)\n",
        "        if(self.pagination_supported):\n",
        "            self.params.update({\"page[size]\":ilevel_page_size,\"page[number]\":\"1\"})\n",
        "        if(self.sort):\n",
        "            self.params.update({\"sort\":self.sort})\n",
        "        if(self.start_time and self.append_supported and self.fetch_method==\"append\"):\n",
        "            if(self.deleted_data):\n",
        "                self.params.update({\"filter[deletionDate]\":\"ge(\" + str(self.start_time) + \")\"})\n",
        "            else:\n",
        "                self.params.update({\"filter[lastModifiedDate]\":\"ge(\" + str(self.start_time) + \")\"})\n",
        "        \n",
        "        # Check to see if an update is needed (but disregard periodData)\n",
        "        if(self.fetch_method == \"full\" and not self.append_supported):\n",
        "            if(self.pagination_supported):\n",
        "                self.params.update({\"page[size]\":\"1\", \"page[number]\":\"1\"})\n",
        "            self.headers = {\"Authorization\": \"Bearer \" + get_access_token()[\"access_token\"]}\n",
        "            try:\n",
        "                if(self.pagination_supported):\n",
        "                    r = requests.get(url=self.url_self, headers=self.headers, params=self.params)   \n",
        "                    ilevel_size = r.json()[\"meta\"][\"count\"]\n",
        "                else:\n",
        "                    r = requests.get(url=self.url_self, headers=self.headers, params=None)\n",
        "                    ilevel_size = len(r.json()[\"data\"])\n",
        "                bq_size = bq_get_number_of_latest_records(self.bigquery_table)\n",
        "            except Exception as e:\n",
        "                \n",
        "                raise Exception(r.url + f\" Error while comparing bigquery and ilevel sizes: {e}\") from e\n",
        "\n",
        "            # If update is not needed, return\n",
        "            if(ilevel_size == bq_size):\n",
        "                log(\"No new data found in \" + self.bigquery_table + \" (\" + str(ilevel_size) + \" records), not loading to BigQuery. \")\n",
        "                return\n",
        "            # Else reset the params\n",
        "            if(self.pagination_supported):\n",
        "                self.params.update({\"page[size]\":ilevel_page_size})\n",
        "        \n",
        "        # Loop through each page of result\n",
        "        while True:\n",
        "            try:\n",
        "                self.headers = {\"Authorization\": \"Bearer \" + get_access_token()[\"access_token\"]}\n",
        "                r = requests.get(url=self.url_self, headers=self.headers, params=self.params)   \n",
        "                # If no results, end\n",
        "                if(not r.json().get(\"data\")):\n",
        "                    break\n",
        "\n",
        "                for record in r.json()[\"data\"]:\n",
        "                    self.add_record_to_read_buffer(record)\n",
        "                    \n",
        "                self.subtotal_count = len(r.json()[\"data\"])\n",
        "                \n",
        "                s = \"Extracted \" + str(len(r.json()[\"data\"])) + \" rows from iLevel \" + self.endpoint\n",
        "                if(verbose):\n",
        "                    s += \" \" + verbose\n",
        "                if(self.pagination_supported):\n",
        "                    s += \" \" + parse_qs(urlparse(r.json()[\"links\"][\"self\"]).query)['page[number]'][0] + \"/\" + parse_qs(urlparse(r.json()[\"links\"][\"last\"]).query)['page[number]'][0]\n",
        "                log(s)\n",
        "\n",
        "                # Check for more pages under this filter\n",
        "                next_page = get_next_page(r)\n",
        "                \n",
        "                # If read buffer is full: load to bigquery\n",
        "                if(len(self.read_buffer) >= read_buffer_size):\n",
        "                    self.push_to_bigquery()\n",
        "                    self.read_buffer.clear()\n",
        "                \n",
        "                # End loop if no more pages\n",
        "                if(not next_page or not self.pagination_supported):\n",
        "                    # Push remaining items to bigquery before ending\n",
        "                    if(len(self.read_buffer) > 0):\n",
        "                        self.push_to_bigquery()\n",
        "                        self.read_buffer.clear()\n",
        "                    break\n",
        "                \n",
        "                self.params.update(next_page)\n",
        "\n",
        "            except Exception as e:\n",
        "                raise Exception(f\"Last request: \" + str(r) + \" \" + r.url + \" \" + str(r.json()) + \"{e}\") from e\n",
        "\n",
        "        log(\"Successfully extracted and loaded \" + f'{self.subtotal_count:,}' + \" records from \" + self.endpoint + \" (\" + self.fetch_method + \" records) to BigQuery\")  \n",
        "    \n",
        "    def add_record_to_fetched_data(self, record):\n",
        "        # Need to be defined in abstract children functions\n",
        "        None\n",
        "\n",
        "    def push_to_bigquery(self,truncate=False):\n",
        "        load_data_to_bigquery(table=self.bigquery_table, records=self.read_buffer,truncate=self.truncate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0I6AE3roGvN"
      },
      "source": [
        "## Define endpoints (Abstract children)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXCbUqx1B_td"
      },
      "source": [
        "### periodicData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE1evTBzCKdV"
      },
      "outputs": [],
      "source": [
        "class periodicData(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                    \"fetched_at\":       str(self.fetched_at),\n",
        "                    \"fetched_by\":       str(self.fetched_by),\n",
        "                    \"fetched_method\":   str(self.fetch_method),\n",
        "                    \n",
        "                    \"type\":             str(record.get(\"type\")),\n",
        "                    \"id\":               str(record.get(\"id\")),\n",
        "                    \n",
        "                    \"value\":            str(record[\"attributes\"].get(\"value\")),\n",
        "                    \"dataItem\":         str(record[\"attributes\"].get(\"dataItem\")),\n",
        "                    \"dataValueType\":    str(record[\"attributes\"].get(\"dataValueType\")),\n",
        "                    \"owner\":            str(record[\"attributes\"].get(\"owner\")),\n",
        "                    \"investment\":       str(record[\"attributes\"].get(\"investment\")),\n",
        "                    \"relationshipPath\": str(record[\"attributes\"].get(\"relationshipPath\")),\n",
        "                    \"scenario\":         str(record[\"attributes\"].get(\"scenario\")),\n",
        "                    \"currency\":         str(record[\"attributes\"].get(\"currency\")),\n",
        "                    \"security\":         str(record[\"attributes\"].get(\"security\")),\n",
        "                    \"segment\":          str(record[\"attributes\"].get(\"segment\")),\n",
        "                    \"periodEnd\":        str(record[\"attributes\"].get(\"periodEnd\")),\n",
        "                    \"periodLength\":     str(record[\"attributes\"].get(\"periodLength\")),\n",
        "                    \"asOfDate\":         str(record[\"attributes\"].get(\"asOfDate\")),\n",
        "                    \"lastModifiedDate\":  str(record[\"attributes\"].get(\"lastModifiedDate\"))\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PHtRLYVCN06"
      },
      "source": [
        "### assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwUIjW1TCQQY"
      },
      "outputs": [],
      "source": [
        "class assets(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":                 str(self.fetched_at),\n",
        "                \"fetched_by\":                 str(self.fetched_by),\n",
        "                \"fetched_method\":             str(self.fetch_method),\n",
        "                \n",
        "                \"type\":                       str(record[\"type\"]),\n",
        "                \"id\":                         str(record[\"id\"]),\n",
        "                \n",
        "                \"name\":                       str(record[\"attributes\"].get(\"name\")),\n",
        "                \"excelTicker\":                str(record[\"attributes\"].get(\"excelTicker\")),\n",
        "                \"status\":                     str(record[\"attributes\"].get(\"status\")),\n",
        "                \"assetType\":                  str(record[\"attributes\"].get(\"assetType\")),\n",
        "                \"reportingCurrency\":          str(record[\"attributes\"].get(\"reportingCurrency\")),\n",
        "                \"geography\":                  str(record[\"attributes\"].get(\"geography\")),\n",
        "                \"industry\":                   str(record[\"attributes\"].get(\"industry\")),\n",
        "                \"calendarType\":               str(record[\"attributes\"].get(\"calendarType\")),\n",
        "                \"longDescription\":            str(record[\"attributes\"].get(\"longDescription\")),\n",
        "                \"shortDescription\":           str(record[\"attributes\"].get(\"shortDescription\")),\n",
        "                \"acquisitionDate\":            str(record[\"attributes\"].get(\"acquisitionDate\")),\n",
        "                \"acquisitionAsOf\":            str(record[\"attributes\"].get(\"acquisitionAsOf\")),\n",
        "                \"exitDate\":                   str(record[\"attributes\"].get(\"exitDate\")),\n",
        "                \"exitAsOf\":                   str(record[\"attributes\"].get(\"exitAsOf\")),\n",
        "                \"leadInvestmentProfessional\": str(record[\"attributes\"].get(\"leadInvestmentProfessional\")),\n",
        "                \"initialPeriod\":              str(record[\"attributes\"].get(\"initialPeriod\")),\n",
        "                \"isPublic\":                   str(record[\"attributes\"].get(\"isPublic\")),\n",
        "                \"website\":                    str(record[\"attributes\"].get(\"website\")),\n",
        "                \"lastModifiedDate\":            str(record[\"attributes\"].get(\"lastModifiedDate\"))\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKzVacrJCkz8"
      },
      "source": [
        "### cashTransactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKT12_MLCeFj"
      },
      "outputs": [],
      "source": [
        "class cashTransactions(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":          str(self.fetched_at),\n",
        "                \"fetched_by\":          str(self.fetched_by),\n",
        "                \"fetched_method\":      str(self.fetch_method),\n",
        "\n",
        "                \"type\":                str(record[\"type\"]),\n",
        "                \"id\":                  str(record[\"id\"]),\n",
        "\n",
        "                \"baseAmount\":          str(record[\"attributes\"].get(\"baseAmount\")),\n",
        "                \"baseCurrency\":        str(record[\"attributes\"].get(\"baseCurrency\")),\n",
        "                \"owner\":               str(record[\"attributes\"].get(\"owner\")),\n",
        "                \"investment\":          str(record[\"attributes\"].get(\"investment\")),\n",
        "                \"security\":            str(record[\"attributes\"].get(\"security\")),\n",
        "                \"scenario\":            str(record[\"attributes\"].get(\"scenario\")),\n",
        "                \"transactionType\":     str(record[\"attributes\"].get(\"transactionType\")),\n",
        "                \"transactionCategory\": str(record[\"attributes\"].get(\"transactionCategory\")),\n",
        "                \"transactionDate\":     str(record[\"attributes\"].get(\"transactionDate\")),\n",
        "                \"asOfDate\":            str(record[\"attributes\"].get(\"asOfDate\")),\n",
        "                \"localAmount\":         str(record[\"attributes\"].get(\"localAmount\")),\n",
        "                \"localCurrency\":       str(record[\"attributes\"].get(\"localCurrency\")),\n",
        "                \"groupId\":             str(record[\"attributes\"].get(\"groupId\")),\n",
        "                \"originalId\":          str(record[\"attributes\"].get(\"originalId\")),\n",
        "                \"description\":         str(record[\"attributes\"].get(\"description\")),\n",
        "                \"lastModifiedBy\":       str(record[\"attributes\"].get(\"lastModifiedBy\")),\n",
        "                \"lastModifiedDate\":     str(record[\"attributes\"].get(\"lastModifiedDate\"))\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRoZY7QsCngU"
      },
      "source": [
        "### cashTransactionTypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo_HPPA_Cryk"
      },
      "outputs": [],
      "source": [
        "class cashTransactionTypes(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":          str(self.fetched_at),\n",
        "                \"fetched_by\":          str(self.fetched_by),\n",
        "                \"fetched_method\":      str(self.fetch_method),\n",
        "\n",
        "                \"type\":                str(record[\"type\"]),\n",
        "                \"id\":                  str(record[\"id\"]),\n",
        "\n",
        "                \"name\":                str(record[\"attributes\"].get(\"name\")),\n",
        "                \"category\":            str(record[\"attributes\"].get(\"category\")),\n",
        "                \"canBeRenamed\":        str(record[\"attributes\"].get(\"canBeRenamed\")),\n",
        "                \"canBeDeleted\":        str(record[\"attributes\"].get(\"canBeDeleted\")),\n",
        "                \"dataItemName\":        str(record[\"attributes\"].get(\"dataItemName\")),\n",
        "                \"isExposed\":           str(record[\"attributes\"].get(\"isExposed\")),\n",
        "                \"isPositiveOperator\":  str(record[\"attributes\"].get(\"isPositiveOperator\")),\n",
        "                \"relationships\":       str(record.get(\"relationships\")).replace(\"None\",\"\\'None\\'\")\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j-gQK8ZCuXX"
      },
      "source": [
        "### currencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZzLFO4-CySm"
      },
      "outputs": [],
      "source": [
        "class currencies(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":     str(self.fetched_at),\n",
        "                \"fetched_by\":     str(self.fetched_by),\n",
        "                \"fetched_method\": str(self.fetch_method),\n",
        "\n",
        "                \"type\":           str(record[\"type\"]),\n",
        "                \"id\":             str(record[\"id\"]),\n",
        "\n",
        "                \"name\":           str(record[\"attributes\"].get(\"name\")),\n",
        "                \"symbol\":         str(record[\"attributes\"].get(\"symbol\"))\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiKjt1UlC07s"
      },
      "source": [
        "### dataItemCategories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY2xD-nHC3rY"
      },
      "outputs": [],
      "source": [
        "class dataItemCategories(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":     str(self.fetched_at),\n",
        "                \"fetched_by\":     str(self.fetched_by),\n",
        "                \"fetched_method\": str(self.fetch_method),\n",
        "\n",
        "                \"type\":           str(record[\"type\"]),\n",
        "                \"id\":             str(record[\"id\"]),\n",
        "\n",
        "                \"name\":           str(record[\"attributes\"].get(\"name\")),\n",
        "                \"sequenceNumber\": str(record[\"attributes\"].get(\"sequenceNumber\")),\n",
        "                \"relationships\":  str(record.get(\"relationships\")).replace(\"None\",\"\\'None\\'\")\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKlrwbWAC5zz"
      },
      "source": [
        "### deals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LOwxkk-C8DJ"
      },
      "outputs": [],
      "source": [
        "class deals(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":       str(self.fetched_at),\n",
        "                \"fetched_by\":       str(self.fetched_by),\n",
        "                \"fetched_method\":   str(self.fetch_method),\n",
        "\n",
        "                \"type\":             str(record[\"type\"]),\n",
        "                \"id\":               str(record[\"id\"]),\n",
        "\n",
        "                \"dealType\":         str(record[\"attributes\"].get(\"dealType\")),\n",
        "                \"acquisitionDate\":  str(record[\"attributes\"].get(\"acquisitionDate\")),\n",
        "                \"investmentName\":   str(record[\"attributes\"].get(\"investmentName\")),\n",
        "                \"investmentAmount\": str(record[\"attributes\"].get(\"investmentAmount\")),\n",
        "                \"currency\":         str(record[\"attributes\"].get(\"currency\")),\n",
        "                \"exitDate\":         str(record[\"attributes\"].get(\"exitDate\")),\n",
        "                \"status\":           str(record[\"attributes\"].get(\"status\")),\n",
        "                \"ownerName\":        str(record[\"attributes\"].get(\"ownerName\")),\n",
        "                \"ownership\":        str(record[\"attributes\"].get(\"ownership\")),\n",
        "                \"securityName\":     str(record[\"attributes\"].get(\"securityName\")),\n",
        "                \"lastModifiedDate\":  str(record[\"attributes\"].get(\"lastModifiedDate\")),\n",
        "                \"relationships\":    str(record.get(\"relationships\")).replace(\"None\",\"\\'None\\'\")\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE9ztCY0C-G6"
      },
      "source": [
        "### documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnfOiIjHC94c"
      },
      "outputs": [],
      "source": [
        "class documents(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":             str(self.fetched_at),\n",
        "                \"fetched_by\":             str(self.fetched_by),\n",
        "                \"fetched_method\":         str(self.fetch_method),\n",
        "\n",
        "                \"type\":                   str(record[\"type\"]),\n",
        "                \"id\":                     str(record[\"id\"]),\n",
        "\n",
        "                \"name\":                   str(record[\"attributes\"].get(\"name\")),\n",
        "                \"extension\":              str(record[\"attributes\"].get(\"extension\")),\n",
        "                \"size\":                   str(record[\"attributes\"].get(\"size\")),\n",
        "                \"tags\":                   str(record[\"attributes\"].get(\"tags\")),\n",
        "                \"periodEnd\":              str(record[\"attributes\"].get(\"periodEnd\")),\n",
        "                \"periodLength\":           str(record[\"attributes\"].get(\"periodLength\")),\n",
        "                \"eventDate\":              str(record[\"attributes\"].get(\"eventDate\")),\n",
        "                \"location\":               str(record[\"attributes\"].get(\"location\")),\n",
        "                \"uploadedBy\":             str(record[\"attributes\"].get(\"uploadedBy\")),\n",
        "                \"uploadDate\":             str(record[\"attributes\"].get(\"uploadDate\")),\n",
        "                \"availableForPublishing\": str(record[\"attributes\"].get(\"availableForPublishing\")),\n",
        "                \"isAvailableToAllUsers\":  str(record[\"attributes\"].get(\"isAvailableToAllUsers\")),\n",
        "                \"lastModifiedBy\":          str(record[\"attributes\"].get(\"lastModifiedBy\")),\n",
        "                \"lastModifiedDate\":        str(record[\"attributes\"].get(\"lastModifiedDate\")),\n",
        "                \"relationships\":          str(record.get(\"relationships\")).replace(\"None\",\"\\'None\\'\")\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-NgQiMGDCoV"
      },
      "source": [
        "### entityGroupCategories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkfoPA9YDCf_"
      },
      "outputs": [],
      "source": [
        "class entityGroupCategories(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":      str(self.fetched_at),\n",
        "                \"fetched_by\":      str(self.fetched_by),\n",
        "                \"fetched_method\":  str(self.fetch_method),\n",
        "\n",
        "                \"type\":            str(record[\"type\"]),\n",
        "                \"id\":              str(record[\"id\"]),\n",
        "\n",
        "                \"name\":            str(record[\"attributes\"].get(\"name\")),\n",
        "                \"entityType\":      str(record[\"attributes\"].get(\"entityType\")),\n",
        "                \"groupDefinition\":  str(record[\"attributes\"].get(\"groupDefinition\")),\n",
        "                \"groupDefinedBy\":   str(record[\"attributes\"].get(\"groupDefinedBy\")),\n",
        "                \"applicableTo\":    str(record[\"attributes\"].get(\"applicableTo\")),\n",
        "                \"relationships\":   str(record.get(\"relationships\")).replace(\"None\",\"\\'None\\'\")\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-NBxHA2DHxZ"
      },
      "source": [
        "### entityGroups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L_8bBKxDHly"
      },
      "outputs": [],
      "source": [
        "class entityGroups(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":         str(self.fetched_at),\n",
        "                \"fetched_by\":         str(self.fetched_by),\n",
        "                \"fetched_method\":     str(self.fetch_method),\n",
        "\n",
        "                \"type\":               str(record[\"type\"]),\n",
        "                \"id\":                 str(record[\"id\"]),\n",
        "\n",
        "                \"name\":               str(record[\"attributes\"].get(\"name\")),\n",
        "                \"entityCategoryType\": str(record[\"attributes\"].get(\"entityCategoryType\")),\n",
        "                \"entityIds\":          str(record[\"attributes\"].get(\"entityIds\")),\n",
        "                \"applicableTo\":       str(record[\"attributes\"].get(\"applicableTo\")),\n",
        "                \"entitiesCount\":      str(record[\"attributes\"].get(\"entitiesCount\")),\n",
        "                \"relationships\":      str(record.get(\"relationships\")).replace(\"None\",\"\\'None\\'\")\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chWVMPlsDMk1"
      },
      "source": [
        "### folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFrSbWdhDOV4"
      },
      "outputs": [],
      "source": [
        "class folders(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":       str(self.fetched_at),\n",
        "                \"fetched_by\":       str(self.fetched_by),\n",
        "                \"fetched_method\":   str(self.fetch_method),\n",
        "\n",
        "                \"type\":             str(record[\"type\"]),\n",
        "                \"id\":               str(record[\"id\"]),\n",
        "\n",
        "                \"name\":             str(record[\"attributes\"].get(\"name\")),\n",
        "                \"createdDate\":      str(record[\"attributes\"].get(\"createdDate\")),\n",
        "                \"lastModifiedDate\":  str(record[\"attributes\"].get(\"lastModifiedDate\")),\n",
        "                \"relationships\":    str(record.get(\"relationships\")).replace(\"None\",\"\\'None\\'\")\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjvbvfQpDQCV"
      },
      "source": [
        "### funds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr1R0xACDRqa"
      },
      "outputs": [],
      "source": [
        "class funds(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":                 str(self.fetched_at),\n",
        "                \"fetched_by\":                 str(self.fetched_by),\n",
        "                \"fetched_method\":             str(self.fetch_method),\n",
        "\n",
        "                \"type\":                       str(record[\"type\"]),\n",
        "                \"id\":                         str(record[\"id\"]),\n",
        "\n",
        "                \"name\":                       str(record[\"attributes\"].get(\"name\")),\n",
        "                \"longName\":                   str(record[\"attributes\"].get(\"longName\")),\n",
        "                \"status\":                     str(record[\"attributes\"].get(\"status\")),\n",
        "                \"ownerType\":                  str(record[\"attributes\"].get(\"ownerType\")),\n",
        "                \"reportingCurrency\":          str(record[\"attributes\"].get(\"reportingCurrency\")),\n",
        "                \"vintage\":                    str(record[\"attributes\"].get(\"vintage\")),\n",
        "                \"totalCommittedCapital\":      str(record[\"attributes\"].get(\"totalCommittedCapital\")),\n",
        "                \"initialCloseDate\":           str(record[\"attributes\"].get(\"initialCloseDate\")),\n",
        "                \"finalCloseDate\":              str(record[\"attributes\"].get(\"finalCloseDate\")),\n",
        "                \"includeInWorkflow\":           str(record[\"attributes\"].get(\"includeInWorkflow\")),\n",
        "                \"description\":                str(record[\"attributes\"].get(\"description\")),\n",
        "                \"calendarType\":               str(record[\"attributes\"].get(\"calendarType\")),\n",
        "                \"initialPeriod\":              str(record[\"attributes\"].get(\"initialPeriod\")),\n",
        "                \"defaultPmeIndex\":            str(record[\"attributes\"].get(\"defaultPmeIndex\")),\n",
        "                \"commitment\":                 str(record[\"attributes\"].get(\"commitment\")),\n",
        "                \"defaultPmeLiquidityPremium\": str(record[\"attributes\"].get(\"defaultPmeLiquidityPremium\")),\n",
        "                \"generalPartner\":             str(record[\"attributes\"].get(\"generalPartner\")),\n",
        "                \"subStrategy\":                str(record[\"attributes\"].get(\"subStrategy\")),\n",
        "                \"strategy\":                   str(record[\"attributes\"].get(\"strategy\")),\n",
        "                \"fundSize\":                   str(record[\"attributes\"].get(\"fundSize\")),\n",
        "                \"benchmarkVintage\":           str(record[\"attributes\"].get(\"benchmarkVintage\")),\n",
        "                \"lastModifiedDate\":            str(record[\"attributes\"].get(\"lastModifiedDate\")),\n",
        "                \"relationships\":              str(record.get(\"relationships\")).replace(\"None\",\"\\'None\\'\")\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op8n9J5KDVKh"
      },
      "source": [
        "### scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgrTYRymDXjU"
      },
      "outputs": [],
      "source": [
        "class scenarios(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":       str(self.fetched_at),\n",
        "                \"fetched_by\":       str(self.fetched_by),\n",
        "                \"fetched_method\":   str(self.fetch_method),\n",
        "\n",
        "                \"type\":             str(record[\"type\"]),\n",
        "                \"id\":               str(record[\"id\"]),\n",
        "\n",
        "                \"name\":             str(record[\"attributes\"].get(\"name\")),\n",
        "                \"shortName\":        str(record[\"attributes\"].get(\"shortName\")),\n",
        "                \"lastModifiedDate\": str(record[\"attributes\"].get(\"lastModifiedDate\")),\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### securities"
      ],
      "metadata": {
        "id": "6OWMJIUkIf6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class securities(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":       str(self.fetched_at),\n",
        "                \"fetched_by\":       str(self.fetched_by),\n",
        "                \"fetched_method\":   str(self.fetch_method),\n",
        "\n",
        "                \"type\":             str(record[\"type\"]),\n",
        "                \"id\":               str(record[\"id\"]),\n",
        "\n",
        "                \"name\":             str(record[\"attributes\"].get(\"name\")),\n",
        "                \"shortName\":        str(record[\"attributes\"].get(\"shortName\")),\n",
        "                \"securityType\":     str(record[\"attributes\"].get(\"type\")),\n",
        "                \"securitySubType\":  str(record[\"attributes\"].get(\"subtype\")),\n",
        "                \"hasOwnership\":     str(record[\"attributes\"].get(\"hasOwnership\")),\n",
        "                \"isActive\":         str(record[\"attributes\"].get(\"isActive\")),\n",
        "                \"lastModifiedDate\":  str(record[\"attributes\"].get(\"lastModifiedDate\")),\n",
        "                \"relationships\":    str(record.get(\"relationships\")).replace(\"None\",\"\\'None\\'\")\n",
        "                })"
      ],
      "metadata": {
        "id": "4D11gmL_IiXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqP5BVYWDZDP"
      },
      "source": [
        "### segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ2VxiKvDaUf"
      },
      "outputs": [],
      "source": [
        "class segments(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":       str(self.fetched_at),\n",
        "                \"fetched_by\":       str(self.fetched_by),\n",
        "                \"fetched_method\":   str(self.fetch_method),\n",
        "\n",
        "                \"type\":             str(record[\"type\"]),\n",
        "                \"id\":               str(record[\"id\"]),\n",
        "\n",
        "                \"name\":             str(record[\"attributes\"].get(\"name\")),\n",
        "                \"isActive\":         str(record[\"attributes\"].get(\"isActive\")),\n",
        "                \"isAllowRollup\":    str(record[\"attributes\"].get(\"isAllowRollup\")),\n",
        "                \"nullReplacement\":  str(record[\"attributes\"].get(\"nullReplacement\")),\n",
        "                \"isGlobal\":         str(record[\"attributes\"].get(\"isGlobal\")),\n",
        "                \"lastModifiedDate\":  str(record[\"attributes\"].get(\"lastModifiedDate\")),\n",
        "                \"relationships\":    str(record.get(\"relationships\")).replace(\"None\",\"\\'None\\'\")\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saMV8I3IDbrq"
      },
      "source": [
        "### deletedCashTransactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6htWT0RDeo3"
      },
      "outputs": [],
      "source": [
        "class deletedCashTransactions(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":          str(self.fetched_at),\n",
        "                \"fetched_by\":          str(self.fetched_by),\n",
        "                \"fetched_method\":      str(self.fetch_method),\n",
        "\n",
        "                \"type\":                str(record[\"type\"]),\n",
        "                \"id\":                  str(record[\"id\"]),\n",
        "\n",
        "                \"baseAmount\":          str(record[\"attributes\"].get(\"baseAmount\")),\n",
        "                \"baseCurrency\":        str(record[\"attributes\"].get(\"baseCurrency\")),\n",
        "                \"owner\":               str(record[\"attributes\"].get(\"owner\")),\n",
        "                \"investment\":          str(record[\"attributes\"].get(\"investment\")),\n",
        "                \"security\":            str(record[\"attributes\"].get(\"security\")),\n",
        "                \"scenario\":            str(record[\"attributes\"].get(\"scenario\")),\n",
        "                \"transactionType\":     str(record[\"attributes\"].get(\"transactionType\")),\n",
        "                \"transactionCategory\": str(record[\"attributes\"].get(\"transactionCategory\")),\n",
        "                \"transactionDate\":     str(record[\"attributes\"].get(\"transactionDate\")),\n",
        "                \"asOfDate\":            str(record[\"attributes\"].get(\"asOfDate\")),\n",
        "                \"localAmount\":         str(record[\"attributes\"].get(\"localAmount\")),\n",
        "                \"localCurrency\":       str(record[\"attributes\"].get(\"localCurrency\")),\n",
        "                \"valuePerShare\":       str(record[\"attributes\"].get(\"valuePerShare\")),\n",
        "                \"shares\":              str(record[\"attributes\"].get(\"shares\")),\n",
        "                \"costPerShare\":        str(record[\"attributes\"].get(\"costPerShare\")),\n",
        "                \"custom1\":             str(record[\"attributes\"].get(\"custom1\")),\n",
        "                \"custom2\":             str(record[\"attributes\"].get(\"custom2\")),\n",
        "                \"custom3\":             str(record[\"attributes\"].get(\"custom3\")),\n",
        "                \"custom4\":             str(record[\"attributes\"].get(\"custom4\")),\n",
        "                \"custom5\":             str(record[\"attributes\"].get(\"custom5\")),\n",
        "                \"custom6\":             str(record[\"attributes\"].get(\"custom6\")),\n",
        "                \"groupId\":             str(record[\"attributes\"].get(\"groupId\")),\n",
        "                \"originalId\":          str(record[\"attributes\"].get(\"originalId\")),\n",
        "                \"description\":         str(record[\"attributes\"].get(\"description\")),\n",
        "                \"deletedBy\":           str(record[\"attributes\"].get(\"deletedBy\")),\n",
        "                \"deletionDate\":        str(record[\"attributes\"].get(\"deletionDate\")),\n",
        "                \"relationships\":       str(record.get(\"relationships\")).replace(\"None\",\"\\'None\\'\")\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5PQa6k3Dico"
      },
      "source": [
        "### deletedDataItems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BW0PemXTDl6g"
      },
      "outputs": [],
      "source": [
        "class deletedDataItems(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":     str(self.fetched_at),\n",
        "                \"fetched_by\":     str(self.fetched_by),\n",
        "                \"fetched_method\": str(self.fetch_method),\n",
        "\n",
        "                \"type\":           str(record[\"type\"]),\n",
        "                \"id\":             str(record[\"id\"]),\n",
        "\n",
        "                \"name\":           str(record[\"attributes\"].get(\"name\")),\n",
        "                \"dataItemType\":   str(record[\"attributes\"].get(\"dataItemType\")),\n",
        "                \"isCarryOver\":    str(record[\"attributes\"].get(\"isCarryOver\")),\n",
        "                \"isGlobal\":       str(record[\"attributes\"].get(\"isGlobal\")),\n",
        "                \"deletedBy\":      str(record[\"attributes\"].get(\"deletedBy\")),\n",
        "                \"deletionDate\":   str(record[\"attributes\"].get(\"deletionDate\"))\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0k1m0KFDn3G"
      },
      "source": [
        "### deletedDeals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tPXfOIcDpdb"
      },
      "outputs": [],
      "source": [
        "class deletedDeals(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":     str(self.fetched_at),\n",
        "                \"fetched_by\":     str(self.fetched_by),\n",
        "                \"fetched_method\": str(self.fetch_method),\n",
        "\n",
        "                \"type\":           str(record[\"type\"]),\n",
        "                \"id\":             str(record[\"id\"]),\n",
        "\n",
        "                \"deletedBy\":      str(record[\"attributes\"].get(\"deletedBy\")),\n",
        "                \"deletedDate\":    str(record[\"attributes\"].get(\"deletedDate\")),\n",
        "                \"relationships\":  str(record.get(\"relationships\")).replace(\"None\",\"\\'None\\'\")\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXP42cneDrex"
      },
      "source": [
        "### deletedEntities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1LIhobSDvT7"
      },
      "outputs": [],
      "source": [
        "class deletedEntities(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":    str(self.fetched_at),\n",
        "                \"fetched_by\":    str(self.fetched_by),\n",
        "                \"fetched_method\":str(self.fetch_method),\n",
        "\n",
        "                \"type\":          str(record[\"type\"]),\n",
        "                \"id\":            str(record[\"id\"]),\n",
        "\n",
        "                \"name\":          str(record[\"attributes\"].get(\"name\")),\n",
        "                \"deletedBy\":     str(record[\"attributes\"].get(\"deletedBy\")),\n",
        "                \"deletionDate\":  str(record[\"attributes\"].get(\"deletionDate\")),\n",
        "                \"entityType\":    str(record[\"attributes\"].get(\"entityType\"))\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9vnGYP-Dw-X"
      },
      "source": [
        "### deletedPeriodicData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SyZGz-5DzRJ"
      },
      "outputs": [],
      "source": [
        "class deletedPeriodicData(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":       str(self.fetched_at),\n",
        "                \"fetched_by\":       str(self.fetched_by),\n",
        "                \"fetched_method\":   str(self.fetch_method),\n",
        "\n",
        "                \"type\":             str(record[\"type\"]),\n",
        "                \"id\":               str(record[\"id\"]),\n",
        "\n",
        "                \"value\":            str(record[\"attributes\"].get(\"value\")),\n",
        "                \"dataItem\":         str(record[\"attributes\"].get(\"dataItem\")),\n",
        "                \"dataValueType\":    str(record[\"attributes\"].get(\"dataValueType\")),\n",
        "                \"owner\":            str(record[\"attributes\"].get(\"owner\")),\n",
        "                \"investment\":       str(record[\"attributes\"].get(\"investment\")),\n",
        "                \"relationshipPath\": str(record[\"attributes\"].get(\"relationshipPath\")),\n",
        "                \"scenario\":         str(record[\"attributes\"].get(\"scenario\")),\n",
        "                \"currency\":         str(record[\"attributes\"].get(\"currency\")),\n",
        "                \"security\":         str(record[\"attributes\"].get(\"security\")),\n",
        "                \"segment\":          str(record[\"attributes\"].get(\"segment\")),\n",
        "                \"periodEnd\":        str(record[\"attributes\"].get(\"periodEnd\")),\n",
        "                \"periodLength\":     str(record[\"attributes\"].get(\"periodLength\")),\n",
        "                \"asOfDate\":         str(record[\"attributes\"].get(\"asOfDate\")),\n",
        "                \"lastModifiedDate\":  str(record[\"attributes\"].get(\"lastModifiedDate\")),\n",
        "                \"relationships\":    str(record.get(\"relationships\")).replace(\"None\",\"\\'None\\'\")\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4IWg7gYD1Ig"
      },
      "source": [
        "### deletedScenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX2GyyHzD4qP"
      },
      "outputs": [],
      "source": [
        "class deletedScenarios(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":     str(self.fetched_at),\n",
        "                \"fetched_by\":     str(self.fetched_by),\n",
        "                \"fetched_method\": str(self.fetch_method),\n",
        "\n",
        "                \"type\":           str(record[\"type\"]),\n",
        "                \"id\":             str(record[\"id\"]),\n",
        "\n",
        "                \"name\":           str(record[\"attributes\"].get(\"name\")),\n",
        "                \"deletedBy\":      str(record[\"attributes\"].get(\"deletedBy\")),\n",
        "                \"deletionDate\":   str(record[\"attributes\"].get(\"deletionDate\"))\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LILMGPWeD5ey"
      },
      "source": [
        "### deletedSecurities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jebBICCD7Pp"
      },
      "outputs": [],
      "source": [
        "class deletedSecurities(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "                \"fetched_at\":     str(self.fetched_at),\n",
        "                \"fetched_by\":     str(self.fetched_by),\n",
        "                \"fetched_method\": str(self.fetch_method),\n",
        "\n",
        "                \"type\":           str(record[\"type\"]),\n",
        "                \"id\":             str(record[\"id\"]),\n",
        "\n",
        "                \"name\":           str(record[\"attributes\"].get(\"name\")),\n",
        "                \"shortName\":      str(record[\"attributes\"].get(\"shortName\")),\n",
        "                \"recordType\":     str(record[\"attributes\"].get(\"type\")),\n",
        "                \"subType\":        str(record[\"attributes\"].get(\"subType\")),\n",
        "                \"isActive\":       str(record[\"attributes\"].get(\"isActive\")),\n",
        "                \"hasOwnership\":   str(record[\"attributes\"].get(\"hasOwnership\")),\n",
        "                \n",
        "                \"deletedBy\":      str(record[\"attributes\"].get(\"deletedBy\")),\n",
        "                \"deletionDate\":   str(record[\"attributes\"].get(\"deletionDate\"))\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grJDiUfYD827"
      },
      "source": [
        "### deletedSegments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsIHJY1PgNLa"
      },
      "outputs": [],
      "source": [
        "class deletedSegments(ilevel_to_bigquery):\n",
        "    def add_record_to_read_buffer(self, record):\n",
        "        self.read_buffer.append({\n",
        "            \"fetched_at\":     str(self.fetched_at),\n",
        "            \"fetched_by\":     str(self.fetched_by),\n",
        "            \"fetched_method\": str(self.fetch_method),\n",
        "\n",
        "            \"type\":           str(record[\"type\"]),\n",
        "            \"id\":             str(record[\"id\"]),\n",
        "\n",
        "            \"name\":           str(record[\"attributes\"].get(\"name\")),\n",
        "            \n",
        "            \"deletedBy\":      str(record[\"attributes\"].get(\"deletedBy\")),\n",
        "            \"deletionDate\":   str(record[\"attributes\"].get(\"deletionDate\"))\n",
        "            })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GJ-Fdaeqr7t"
      },
      "source": [
        "## Tail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9UBRoseqqcB"
      },
      "outputs": [],
      "source": [
        "def tail():\n",
        "    log(\"Script finished adding \" + f'{total_count:,}' + \" to BigQuery\")\n",
        "\n",
        "    stop = timeit.default_timer()\n",
        "    total_time = stop - start\n",
        "\n",
        "    # output running time in a nice format.\n",
        "    mins, secs = divmod(total_time, 60)\n",
        "    hours, mins = divmod(mins, 60)\n",
        "\n",
        "    log(\"Total running time: %d hours, %d minutes, %d seconds.\" % (hours, mins, secs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhvFV5VpoJYB"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBCUSKoAfrQc"
      },
      "outputs": [],
      "source": [
        "def main(x,y):\n",
        "    head()\n",
        "\n",
        "    build_lists_of_assets_scenarios_dataitems()\n",
        "\n",
        "    # Run all the endpoints\n",
        "    periodicData           (endpoint=\"periodicData\",           sort=\"-lastModifiedDate\",append_supported=True, filters=get_filter_list(list_assets, list_scenarios, list_dataItems))\n",
        "    assets                 (endpoint=\"entities/assets\",        sort=\"name\")\n",
        "    cashTransactions       (endpoint=\"cashTransactions\",       sort=\"-lastModifiedDate\")\n",
        "    cashTransactionTypes   (endpoint=\"cashTransactionTypes\",   sort=None,pagination_supported=False)\n",
        "    currencies             (endpoint=\"currencies\",             sort=None,pagination_supported=False)\n",
        "    dataItemCategories     (endpoint=\"dataItemCategories\",     sort=\"name\")\n",
        "    deals                  (endpoint=\"deals\",                  sort=\"-lastModifiedDate\")\n",
        "    documents              (endpoint=\"documents\",              sort=\"-lastModifiedDate\")\n",
        "    entityGroupCategories  (endpoint=\"entityGroupCategories\",  sort=None,pagination_supported=False)\n",
        "    entityGroups           (endpoint=\"entityGroups\",           sort=None,pagination_supported=False)\n",
        "    folders                (endpoint=\"folders\",                sort=\"name\")\n",
        "    funds                  (endpoint=\"entities/funds\",         sort=\"-lastModifiedDate\")\n",
        "    scenarios              (endpoint=\"scenarios\",              sort=None,pagination_supported=False)\n",
        "    securities             (endpoint=\"securities\",             sort=\"-lastModifiedDate\")\n",
        "    segments               (endpoint=\"segments\",               sort=\"-lastModifiedDate\")\n",
        "    deletedCashTransactions(endpoint=\"deletedCashTransactions\",sort=\"deletionDate\",deleted_data=True)\n",
        "    deletedDataItems       (endpoint=\"deletedDataItems\",       sort=\"deletionDate\",deleted_data=True)\n",
        "    deletedDeals           (endpoint=\"deletedDeals\",           sort=\"deletionDate\",deleted_data=True)\n",
        "    deletedEntities        (endpoint=\"deletedEntities\",        sort=\"deletionDate\",deleted_data=True)\n",
        "    deletedPeriodicData    (endpoint=\"deletedPeriodicData\",    sort=\"-lastModifiedDate\")\n",
        "    deletedScenarios       (endpoint=\"deletedScenarios\",       sort=\"deletionDate\",deleted_data=True)\n",
        "    deletedSecurities      (endpoint=\"deletedSecurities\",      sort=\"deletionDate\",deleted_data=True)\n",
        "    deletedSegments        (endpoint=\"deletedSegments\",        sort=\"deletionDate\",deleted_data=True)\n",
        "\n",
        "    tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To be deleted when running in cloud functions"
      ],
      "metadata": {
        "id": "0W4eAQRYr6qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main(\"x\",\"y\")"
      ],
      "metadata": {
        "id": "EjP3yCjPr5iU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JNSiY1T93ONb",
        "y6bN09VIU1dI",
        "H7mMZAPymedK",
        "phZ4XA7Fmin2",
        "Ws03A4HAmslC",
        "fasnHJyKmt7y",
        "7lpWXZBNmvzG",
        "M4u5IXg4DXwE",
        "s6KPtrzEm4mG",
        "qoKanOKHRKH7",
        "S4OBckTGBUHG",
        "U4VvZJ8v1OJt",
        "g0I6AE3roGvN",
        "hXCbUqx1B_td",
        "4PHtRLYVCN06",
        "DKzVacrJCkz8",
        "op8n9J5KDVKh",
        "r5PQa6k3Dico",
        "NXP42cneDrex",
        "-4IWg7gYD1Ig",
        "LILMGPWeD5ey",
        "grJDiUfYD827",
        "9GJ-Fdaeqr7t"
      ],
      "name": "ilevel-to-bigquery.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}